# ğŸ“˜ Week 5: SMOTE-Based Resampling and Model Retraining

---

## ğŸ¯ Objective

To handle **extreme class imbalance** in the dataset using **SMOTE (Synthetic Minority Oversampling Technique)** and retrain models to evaluate if **recallâ€“precisionâ€“F1 trade-offs** improve compared to the **baseline (Week 4)**.

---

## ğŸ§ª Methodology

- Applied **SMOTE only on training data**
- Retained original test set for **unbiased evaluation**
- Trained three classifiers:
  - âœ… Logistic Regression
  - âœ… Decision Tree
  - âœ… Random Forest
- Compared performance with **baseline imbalanced models**

---

## ğŸ“Š Model Performance After SMOTE

### ğŸ”¹ Logistic Regression + SMOTE

| **Metric**       | **Value** | **Observation**                         |
|------------------|-----------|------------------------------------------|
| **Precision (1)**| 0.0581    | âŒ Very low â€” high false alarms           |
| **Recall (1)**   | 0.9184    | âœ… Excellent â€” caught nearly all frauds   |
| **F1-score (1)** | 0.1094    | âŒ Poor balance                          |
| **ROC-AUC**      | 0.9698    | âœ… Very good                             |

ğŸ“Œ **Conclusion**: ğŸ”º Recall-focused but not practical due to **high false positives**

---

### ğŸ”¹ Decision Tree + SMOTE

| **Metric**       | **Value** | **Observation**                          |
|------------------|-----------|-------------------------------------------|
| **Precision (1)**| 0.3393    | âš ï¸ Much lower than baseline               |
| **Recall (1)**   | 0.7755    | âœ… Improved                               |
| **F1-score (1)** | 0.4720    | âš ï¸ Decreased due to precision drop        |
| **ROC-AUC**      | 0.8865    | âœ… Improved                               |

ğŸ“Œ **Conclusion**: Balanced recall, but **huge spike in false positives** hurts usability

---

### ğŸ”¹ Random Forest + SMOTE

| **Metric**       | **Value** | **Observation**                          |
|------------------|-----------|-------------------------------------------|
| **Precision (1)**| 0.8144    | âœ… High â€” most predicted frauds were correct |
| **Recall (1)**   | 0.8061    | âœ… High â€” caught 80% of frauds             |
| **F1-score (1)** | 0.8103    | âœ… Best balance overall                    |
| **ROC-AUC**      | 0.9688    | âœ… Excellent                               |

ğŸ“Œ **Conclusion**: â­ **Best performing model** â€” strong in all metrics with minimal trade-offs

---

## ğŸ“Œ Key Takeaways

- âœ… **SMOTE** helped **increase recall** in all models.
- âŒ **Logistic Regression** remained too **imprecise** to be useful.
- âš ï¸ **Decision Tree** became **too aggressive**, leading to **many false alarms**.
- ğŸŒ² **Random Forest with SMOTE** struck the **best balance**:
  - âš–ï¸ High **recall** and **precision**
  - ğŸ’¡ Strong **F1-score** and **ROC-AUC**
  - âœ… **Recommended as the new baseline** going forward

